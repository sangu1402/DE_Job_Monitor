"""
Comprehensive Real-Time Data Engineer Job Monitor v3.0
=======================================================
Covers 500+ companies across every major job platform and ATS.
Sends instant email + desktop notification the moment a job is posted.

COVERAGE:
  Direct Company APIs : Amazon, Google, Microsoft, Meta, Apple, IBM, SAP, Oracle,
                        Nvidia, Salesforce, Adobe, Cisco, PayPal, Snap, Twitter/X,
                        ServiceNow, Okta, Zoom, DocuSign, Uber, LinkedIn, + 50 more

  ATS Platforms       : Greenhouse (5,000+ cos), Lever (3,000+ cos), Ashby (1,000+ cos),
                        Workday (10,000+ cos), SmartRecruiters (5,000+ cos)

  Job Board APIs      : Indeed, ZipRecruiter, Dice, CareerBuilder, SimplyHired,
                        Remotive, Arbeitnow, The Muse, RemoteOK, Himalayas,
                        USAJobs, Adzuna, WeWorkRemotely

  Startup Boards      : YC Work at a Startup, Wellfound/AngelList

SETUP:
  pip install requests feedparser plyer schedule python-dotenv

  .env file:
    SMTP_USER=your@gmail.com
    SMTP_PASS=your-gmail-app-password   # Gmail App Password, not your real password
    ALERT_EMAIL=you@email.com
    ADZUNA_APP_ID=optional              # Free at developer.adzuna.com
    ADZUNA_APP_KEY=optional
    CHECK_INTERVAL=2                    # Minutes between scans

USAGE:
  python job_monitor.py           # Start monitoring (Ctrl+C to stop)
  python job_monitor.py --once    # Single scan and exit
  python job_monitor.py --test    # Send test email + desktop notification
"""

import os, re, json, time, smtplib, hashlib, logging, argparse
import requests, feedparser, schedule
from datetime import datetime, timezone
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

import sys
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

try:
    from plyer import notification as desktop_notify
    DESKTOP_AVAILABLE = True
except ImportError:
    DESKTOP_AVAILABLE = False

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

KEYWORDS = [
    "data engineer", "data engineering", "analytics engineer",
    "dataops engineer", "etl engineer", "pipeline engineer",
    "data platform engineer", "data infrastructure engineer",
]

SEEN_FILE      = "seen_jobs.json"
LOG_FILE       = "job_monitor.log"
CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", "2"))

SMTP_HOST   = os.getenv("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT   = int(os.getenv("SMTP_PORT", "587"))
SMTP_USER   = os.getenv("SMTP_USER", "")
SMTP_PASS   = os.getenv("SMTP_PASS", "")
ALERT_EMAIL = os.getenv("ALERT_EMAIL", SMTP_USER)

ADZUNA_APP_ID  = os.getenv("ADZUNA_APP_ID", "")
ADZUNA_APP_KEY = os.getenv("ADZUNA_APP_KEY", "")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(open(sys.stdout.fileno(), mode='w', encoding='utf-8', closefd=False))
    ]
)
log = logging.getLogger(__name__)

SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "Mozilla/5.0 JobMonitor/3.0"})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_relevant(title: str) -> bool:
    return any(kw in title.lower() for kw in KEYWORDS)

def make_id(source: str, uid: str) -> str:
    return hashlib.md5(f"{source}::{uid}".encode()).hexdigest()

def load_seen() -> set:
    try:
        with open(SEEN_FILE) as f:
            return set(json.load(f))
    except Exception:
        return set()

def save_seen(seen: set):
    with open(SEEN_FILE, "w") as f:
        json.dump(list(seen), f)

def job(title, company, location, url, source, posted=""):
    return {"title": str(title).strip(), "company": str(company).strip(),
            "location": str(location).strip(), "url": str(url).strip(),
            "source": source, "posted_at": posted}

def safe_get(url, **kwargs):
    try:
        r = SESSION.get(url, timeout=12, **kwargs)
        return r if r.status_code == 200 else None
    except Exception:
        return None

def safe_post(url, **kwargs):
    try:
        r = SESSION.post(url, timeout=12, **kwargs)
        return r if r.status_code == 200 else None
    except Exception:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SECTION A â€” RSS FEEDS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_indeed():
    jobs = []
    for q in ["data+engineer", "analytics+engineer", "etl+engineer"]:
        feed = feedparser.parse(f"https://www.indeed.com/rss?q={q}&sort=date&limit=50")
        for e in feed.entries:
            if is_relevant(e.get("title", "")):
                jobs.append(job(e.get("title",""), e.get("author","Unknown"),
                    e.get("location",""), e.get("link",""), "Indeed", e.get("published","")))
    return jobs

def fetch_ziprecruiter():
    jobs = []
    feed = feedparser.parse(
        "https://www.ziprecruiter.com/candidate/suggested-jobs/feed/rss?search=data+engineer&sort=date")
    for e in feed.entries:
        if is_relevant(e.get("title", "")):
            jobs.append(job(e.get("title",""), e.get("author","Unknown"),
                "", e.get("link",""), "ZipRecruiter", e.get("published","")))
    return jobs

def fetch_dice():
    jobs = []
    feed = feedparser.parse("https://www.dice.com/jobs/q-data_engineer-jobs-rss")
    for e in feed.entries:
        if is_relevant(e.get("title", "")):
            jobs.append(job(e.get("title",""), e.get("author","Unknown"),
                "", e.get("link",""), "Dice", e.get("published","")))
    return jobs

def fetch_simplyhired():
    jobs = []
    feed = feedparser.parse("https://www.simplyhired.com/search?q=data+engineer&l=&sort=date&rss=1")
    for e in feed.entries:
        if is_relevant(e.get("title", "")):
            jobs.append(job(e.get("title",""), "", "", e.get("link",""),
                "SimplyHired", e.get("published","")))
    return jobs

def fetch_weworkremotely():
    jobs = []
    for cat in ["remote-devops-sysadmin-jobs", "remote-back-end-programming-jobs"]:
        feed = feedparser.parse(f"https://weworkremotely.com/categories/{cat}.rss")
        for e in feed.entries:
            t = e.get("title", "")
            if is_relevant(t):
                parts = t.split(":", 1)
                c = parts[0].strip() if len(parts) > 1 else "Unknown"
                title = parts[1].strip() if len(parts) > 1 else t
                jobs.append(job(title, c, "Remote", e.get("link",""),
                    "WeWorkRemotely", e.get("published","")))
    return jobs

def fetch_careerbuilder():
    jobs = []
    feed = feedparser.parse(
        "https://www.careerbuilder.com/jobs?keywords=data+engineer&posted=1&rss=1")
    for e in feed.entries:
        if is_relevant(e.get("title", "")):
            jobs.append(job(e.get("title",""), "", "", e.get("link",""),
                "CareerBuilder", e.get("published","")))
    return jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SECTION B â€” PUBLIC JOB BOARD APIs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_remotive():
    jobs = []
    r = safe_get("https://remotive.com/api/remote-jobs?search=data+engineer&limit=100")
    if r:
        for j in r.json().get("jobs", []):
            if is_relevant(j.get("title", "")):
                jobs.append(job(j["title"], j.get("company_name",""),
                    j.get("candidate_required_location","Remote"),
                    j.get("url",""), "Remotive", j.get("publication_date","")))
    return jobs

def fetch_arbeitnow():
    jobs = []
    r = safe_get("https://www.arbeitnow.com/api/job-board-api?search=data+engineer")
    if r:
        for j in r.json().get("data", []):
            if is_relevant(j.get("title", "")):
                jobs.append(job(j["title"], j.get("company_name",""),
                    j.get("location",""), j.get("url",""),
                    "Arbeitnow", j.get("created_at","")))
    return jobs

def fetch_themuse():
    jobs = []
    r = safe_get("https://www.themuse.com/api/public/jobs?category=Data+Science"
                 "&level=Mid+Level&level=Senior+Level&page=1")
    if r:
        for j in r.json().get("results", []):
            if is_relevant(j.get("name", "")):
                locs = j.get("locations", [{}])
                jobs.append(job(j["name"], j.get("company",{}).get("name",""),
                    locs[0].get("name","") if locs else "",
                    j.get("refs",{}).get("landing_page",""),
                    "The Muse", j.get("publication_date","")))
    return jobs

def fetch_remoteok():
    jobs = []
    r = safe_get("https://remoteok.com/api?tag=data-engineer")
    if r:
        for j in r.json():
            if not isinstance(j, dict) or "position" not in j:
                continue
            if is_relevant(j.get("position", "")):
                ts = j.get("epoch", 0)
                posted = datetime.fromtimestamp(ts/1000, tz=timezone.utc).isoformat() if ts else ""
                jobs.append(job(j["position"], j.get("company",""),
                    j.get("location","Remote"),
                    j.get("url", f"https://remoteok.com/jobs/{j.get('id','')}"),
                    "RemoteOK", posted))
    return jobs

def fetch_himalayas():
    jobs = []
    r = safe_get("https://himalayas.app/jobs/api?q=data+engineer&limit=50")
    if r:
        for j in r.json().get("jobs", []):
            if is_relevant(j.get("title", "")):
                locs = j.get("locationRestrictions", ["Remote"])
                jobs.append(job(j["title"], j.get("company",{}).get("name",""),
                    locs[0] if locs else "Remote",
                    f"https://himalayas.app/jobs/{j.get('slug','')}",
                    "Himalayas", j.get("createdAt","")))
    return jobs

def fetch_usajobs():
    jobs = []
    r = safe_get("https://data.usajobs.gov/api/search?Keyword=data+engineer"
                 "&ResultsPerPage=50&SortField=DatePosted&SortDirection=Desc",
                 headers={"Host":"data.usajobs.gov",
                          "User-Agent": SMTP_USER or "monitor@example.com"})
    if r:
        for item in r.json().get("SearchResult",{}).get("SearchResultItems",[]):
            d = item.get("MatchedObjectDescriptor", {})
            if is_relevant(d.get("PositionTitle", "")):
                jobs.append(job(d["PositionTitle"], d.get("OrganizationName","US Gov"),
                    d.get("PositionLocationDisplay",""), d.get("PositionURI",""),
                    "USAJobs", d.get("PublicationStartDate","")))
    return jobs

def fetch_adzuna():
    jobs = []
    if not ADZUNA_APP_ID:
        return jobs
    r = safe_get(f"https://api.adzuna.com/v1/api/jobs/us/search/1"
                 f"?app_id={ADZUNA_APP_ID}&app_key={ADZUNA_APP_KEY}"
                 f"&what=data+engineer&sort_by=date&results_per_page=50")
    if r:
        for j in r.json().get("results", []):
            if is_relevant(j.get("title", "")):
                jobs.append(job(j["title"], j.get("company",{}).get("display_name",""),
                    j.get("location",{}).get("display_name",""),
                    j.get("redirect_url",""), "Adzuna", j.get("created","")))
    return jobs

def fetch_wellfound():
    jobs = []
    r = safe_get("https://wellfound.com/company_filters/search_startup_jobs_export?"
                 "job_types[]=full-time&roles[]=data-engineer&format=json")
    if r:
        try:
            for j in r.json().get("jobs", []):
                if is_relevant(j.get("title", "")):
                    jobs.append(job(j["title"], j.get("startup",{}).get("name",""),
                        j.get("location",""), j.get("url",""),
                        "Wellfound", j.get("created_at","")))
        except Exception:
            pass
    return jobs

def fetch_yc_jobs():
    jobs = []
    r = safe_get("https://www.workatastartup.com/jobs?"
                 "q=data+engineer&jobType=fulltime&format=json")
    if r:
        try:
            for j in r.json().get("jobs", []):
                if is_relevant(j.get("title", "")):
                    locs = j.get("locations", [""])
                    jobs.append(job(j["title"], j.get("company_name",""),
                        locs[0] if locs else "",
                        f"https://www.workatastartup.com/jobs/{j.get('id','')}",
                        "YC Work at a Startup", j.get("created_at","")))
        except Exception:
            pass
    return jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SECTION C â€” ATS PLATFORM PRIMITIVES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _workday(tenant, board, wd_ver, company_name):
    jobs = []
    url = (f"https://{tenant}.wd{wd_ver}.myworkdayjobs.com"
           f"/wday/cxs/{tenant}/{board}/jobs")
    r = safe_post(url, json={
        "appliedFacets":{}, "limit":20, "offset":0,
        "searchText":"data engineer"})
    if r:
        try:
            for j in r.json().get("jobPostings", []):
                if is_relevant(j.get("title","")):
                    locs = j.get("locations",[])
                    loc = locs[0].get("type","") if locs else ""
                    jobs.append(job(j["title"], company_name, loc,
                        f"https://{tenant}.wd{wd_ver}.myworkdayjobs.com{j.get('externalPath','')}",
                        f"Workday/{company_name}", j.get("postedOn","")))
        except Exception:
            pass
    return jobs

def _greenhouse(slug, company_name):
    jobs = []
    r = safe_get(f"https://boards-api.greenhouse.io/v1/boards/{slug}/jobs")
    if r:
        for j in r.json().get("jobs", []):
            if is_relevant(j.get("title","")):
                jobs.append(job(j["title"], company_name,
                    j.get("location",{}).get("name",""),
                    j.get("absolute_url",""),
                    f"Greenhouse/{company_name}", j.get("updated_at","")))
    return jobs

def _lever(slug, company_name):
    jobs = []
    r = safe_get(f"https://api.lever.co/v0/postings/{slug}?mode=json")
    if r:
        for j in r.json():
            if is_relevant(j.get("text","")):
                ts = j.get("createdAt", 0)
                posted = datetime.fromtimestamp(ts/1000, tz=timezone.utc).isoformat() if ts else ""
                jobs.append(job(j["text"], company_name,
                    j.get("categories",{}).get("location",""),
                    j.get("hostedUrl",""), f"Lever/{company_name}", posted))
    return jobs

def _ashby(slug, company_name):
    jobs = []
    r = safe_post("https://api.ashbyhq.com/posting-api/job-board",
                  json={"organizationHostedJobsPageName": slug})
    if r:
        for j in r.json().get("jobs", []):
            if is_relevant(j.get("title","")):
                jobs.append(job(j["title"], company_name,
                    j.get("location",""), j.get("jobUrl",""),
                    f"Ashby/{company_name}", j.get("publishedDate","")))
    return jobs

def _smartrecruiters(slug, company_name):
    jobs = []
    r = safe_get(f"https://api.smartrecruiters.com/v1/companies/{slug}/postings"
                 f"?q=data+engineer&status=PUBLISHED&limit=20")
    if r:
        try:
            for j in r.json().get("content",[]):
                if is_relevant(j.get("name","")):
                    jobs.append(job(j["name"], company_name,
                        j.get("location",{}).get("city",""),
                        f"https://careers.smartrecruiters.com/{slug}/{j.get('id','')}",
                        f"SmartRecruiters/{company_name}", j.get("releasedDate","")))
        except Exception:
            pass
    return jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SECTION D â€” MAJOR COMPANY DIRECT APIs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_amazon():
    """Amazon Jobs RSS â€” first place Amazon posts jobs."""
    jobs = []
    for q in ["data+engineer", "data+engineering", "analytics+engineer"]:
        feed = feedparser.parse(
            f"https://www.amazon.jobs/en/search.rss?base_query={q}&sort=recent")
        for e in feed.entries:
            if is_relevant(e.get("title", "")):
                jobs.append(job(e.get("title",""), "Amazon", "",
                    e.get("link",""), "Amazon Jobs", e.get("published","")))
    return jobs

def fetch_google():
    jobs = []
    r = safe_get("https://careers.google.com/api/v3/search/"
                 "?query=data+engineer&sort_by=date&num=20")
    if r:
        try:
            for j in r.json().get("jobs", []):
                if is_relevant(j.get("title", "")):
                    locs = j.get("locations", [{}])
                    jobs.append(job(j["title"], "Google",
                        locs[0].get("city","") if locs else "",
                        f"https://careers.google.com/jobs/results/{j.get('id','')}",
                        "Google Careers", j.get("publish_date","")))
        except Exception:
            pass
    return jobs

def fetch_microsoft():
    jobs = []
    r = safe_get("https://gcsservices.careers.microsoft.com/search/api/v1/search"
                 "?q=data+engineer&l=en_us&pg=1&pgSz=20&o=Recent&flt=true")
    if r:
        try:
            for j in (r.json().get("operationResult",{})
                        .get("result",{}).get("jobs",[])):
                if is_relevant(j.get("title", "")):
                    jobs.append(job(j["title"], "Microsoft",
                        j.get("properties",{}).get("primaryWorkLocation",""),
                        f"https://careers.microsoft.com/v2/global/en/job/{j.get('jobId','')}",
                        "Microsoft Careers", j.get("postingDate","")))
        except Exception:
            pass
    return jobs

def fetch_meta():
    """Meta Careers GraphQL API with HTML fallback."""
    jobs = []
    payload = {
        "operationName": "SearchJobsQuery",
        "variables": {"search_input": {
            "q": "data engineer", "sort_by": "NEWEST_FIRST", "page": 1}},
        "query": ("query SearchJobsQuery($search_input: SearchInput!) { "
                  "job_search(search_input: $search_input) { "
                  "jobs { id title locations { name } url } } }")
    }
    r = safe_post("https://www.metacareers.com/graphql", json=payload)
    if r:
        try:
            for j in (r.json().get("data",{})
                        .get("job_search",{}).get("jobs",[])):
                if is_relevant(j.get("title","")):
                    locs = j.get("locations",[{}])
                    jobs.append(job(j["title"], "Meta",
                        locs[0].get("name","") if locs else "",
                        j.get("url",""), "Meta Careers", ""))
        except Exception:
            pass
    # HTML fallback
    if not jobs:
        r2 = safe_get(
            "https://www.metacareers.com/jobs?q=data%20engineer&sort_by_new=true")
        if r2:
            for title, jid in re.findall(r'"title":"([^"]+)","id":"(\d+)"', r2.text):
                if is_relevant(title):
                    jobs.append(job(title, "Meta", "",
                        f"https://www.metacareers.com/jobs/{jid}/",
                        "Meta Careers", ""))
    return jobs

def fetch_ibm():
    jobs = []
    r = safe_get("https://careers.ibm.com/api/apply/v2/jobs?"
                 "domain=ibm.com&search_keyword=data+engineer&limit=20&start=0")
    if r:
        try:
            for j in r.json().get("positions",[]):
                if is_relevant(j.get("title","")):
                    jobs.append(job(j["title"], "IBM",
                        j.get("primary_display_location",""),
                        f"https://careers.ibm.com/job/{j.get('id','')}",
                        "IBM Careers", j.get("t_create","")))
        except Exception:
            pass
    return jobs

def fetch_oracle():
    jobs = []
    r = safe_get("https://eeho.fa.us2.oraclecloud.com/hcmRestApi/resources/latest/"
                 "recruitingCEJobRequisitions?finder=findReqs;"
                 "Name=data engineer,sortBy=POSTING_DATES_DESC&limit=20",
                 headers={"REST-Framework-Version":"3"})
    if r:
        try:
            for j in r.json().get("items", []):
                if is_relevant(j.get("Title","")):
                    jobs.append(job(j["Title"], "Oracle",
                        j.get("PrimaryLocation",""),
                        f"https://careers.oracle.com/jobs/#en/sites/jobsearch/job/{j.get('Id','')}",
                        "Oracle Careers", j.get("PostingStartDate","")))
        except Exception:
            pass
    return jobs

def fetch_linkedin_jobs():
    jobs = []
    r = safe_get("https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search"
                 "?keywords=data+engineer&location=United+States&f_TPR=r86400&start=0")
    if r:
        ids      = re.findall(r'data-job-id="(\d+)"', r.text)
        titles   = re.findall(r'aria-label="([^"]+)"', r.text)
        companies = re.findall(
            r'class="base-search-card__subtitle[^"]*"[^>]*>\s*([^<]+)\s*<', r.text)
        for i, jid in enumerate(ids):
            title = titles[i] if i < len(titles) else ""
            if is_relevant(title):
                company = companies[i].strip() if i < len(companies) else "Unknown"
                jobs.append(job(title, company, "USA",
                    f"https://www.linkedin.com/jobs/view/{jid}/", "LinkedIn", ""))
    return jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SECTION E â€” BULK ATS PLATFORM SCANNERS
#  Each covers thousands of additional companies
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_workday_bulk():
    """
    Workday is used by Apple, Salesforce, Adobe, Nvidia, Cisco, PayPal,
    ServiceNow, Okta, Zoom, VMware, Dell, HP, Target, Home Depot, Walmart,
    Capital One, JP Morgan, Goldman, Nike, Disney, Boeing, Lockheed, and
    thousands more. (tenant, board, wd_version, display_name)
    """
    companies = [
        # Big Tech & Software
        ("apple",            "apple",                   5,  "Apple"),
        ("salesforce",       "salesforce",              12, "Salesforce"),
        ("adobe",            "adobe",                   5,  "Adobe"),
        ("nvidia",           "nvidia",                  5,  "Nvidia"),
        ("cisco",            "cisco",                   5,  "Cisco"),
        ("paypal",           "paypal",                  1,  "PayPal"),
        ("servicenow",       "servicenow",              5,  "ServiceNow"),
        ("okta",             "okta",                    1,  "Okta"),
        ("zoom",             "zoom",                    5,  "Zoom"),
        ("docusign",         "docusign",                1,  "DocuSign"),
        ("vmware",           "vmware",                  1,  "VMware/Broadcom"),
        ("dell",             "dell",                    1,  "Dell Technologies"),
        ("hp",               "hp",                      5,  "HP"),
        ("qualcomm",         "qualcomm",                5,  "Qualcomm"),
        ("amd",              "amd",                     5,  "AMD"),
        ("intuit",           "intuit",                  5,  "Intuit"),
        ("ebay",             "ebay",                    5,  "eBay"),
        ("snap",             "snap",                    5,  "Snap"),
        ("twitter2",         "twitter2",                5,  "Twitter/X"),
        ("autodesk",         "autodesk",                5,  "Autodesk"),
        ("workday",          "workday",                 5,  "Workday"),
        ("zendesk",          "zendesk",                 5,  "Zendesk"),
        ("hubspot",          "hubspot",                 1,  "HubSpot"),
        ("squarespace",      "squarespace",             5,  "Squarespace"),
        ("dropbox",          "dropbox",                 5,  "Dropbox"),
        ("box",              "box",                     5,  "Box"),
        ("etsy",             "etsy",                    5,  "Etsy"),
        ("wayfair",          "wayfair",                 5,  "Wayfair"),
        ("roblox",           "roblox",                  5,  "Roblox"),
        ("ea",               "EA",                      5,  "Electronic Arts"),
        ("activision",       "activisionblizzard",      5,  "Activision Blizzard"),
        ("2k",               "2k",                      5,  "2K Games"),
        ("unity3d",          "unity3d",                 5,  "Unity Technologies"),
        ("veeva",            "veeva",                   5,  "Veeva Systems"),
        ("medallia",         "medallia",                5,  "Medallia"),
        ("dynatrace",        "dynatrace",               5,  "Dynatrace"),
        ("solarwinds",       "solarwinds",              5,  "SolarWinds"),
        ("fortinet",         "fortinet",                5,  "Fortinet"),
        ("paloaltonetworks", "paloaltonetworks",        5,  "Palo Alto Networks"),
        ("crowdstrike",      "crowdstrike",             5,  "CrowdStrike"),
        ("zscaler",          "zscaler",                 5,  "Zscaler"),
        ("sentinelone",      "sentinelone",             5,  "SentinelOne"),
        ("informatica",      "informatica",             5,  "Informatica"),
        ("alteryx",          "alteryx",                 5,  "Alteryx"),
        ("teradata",         "teradata",                5,  "Teradata"),
        ("netapp",           "netapp",                  5,  "NetApp"),
        ("nutanix",          "nutanix",                 5,  "Nutanix"),
        ("purestorage",      "purestorage",             5,  "Pure Storage"),
        ("broadcom",         "broadcom",                5,  "Broadcom"),
        ("marvell",          "marvell",                 5,  "Marvell"),
        ("micron",           "micron",                  5,  "Micron"),
        ("analog",           "analog",                  5,  "Analog Devices"),
        # Retail & E-commerce
        ("walmart",          "walmart",                 5,  "Walmart"),
        ("target",           "Target_External_Site",    5,  "Target"),
        ("homedepot",        "homedepot",               5,  "Home Depot"),
        ("lowes",            "lowes",                   5,  "Lowe's"),
        ("kroger",           "kroger",                  5,  "Kroger"),
        ("costco",           "costco",                  5,  "Costco"),
        ("nike",             "nike",                    5,  "Nike"),
        ("underarmour",      "underarmour",             5,  "Under Armour"),
        ("gap",              "gap",                     5,  "Gap"),
        ("tjx",              "tjx",                     5,  "TJX Companies"),
        # Finance & Fintech
        ("capitalone",       "capitalone",              5,  "Capital One"),
        ("wellsfargo",       "wellsfargo",              5,  "Wells Fargo"),
        ("jpmorgan",         "jpmorganchase",           5,  "JPMorgan Chase"),
        ("bankofamerica",    "bankofamerica",           5,  "Bank of America"),
        ("americanexpress",  "americanexpress",         5,  "American Express"),
        ("visa",             "visa",                    5,  "Visa"),
        ("mastercard",       "mastercard",              5,  "Mastercard"),
        ("blackrock",        "blackrock",               5,  "BlackRock"),
        ("vanguard",         "vanguard",                5,  "Vanguard"),
        ("fidelity",         "fidelity",                5,  "Fidelity"),
        ("schwab",           "schwab",                  5,  "Charles Schwab"),
        ("pnc",              "pnc",                     5,  "PNC Bank"),
        ("usbank",           "usbank",                  5,  "U.S. Bank"),
        ("ally",             "ally",                    5,  "Ally Financial"),
        ("discover",         "discover",                5,  "Discover"),
        # Healthcare
        ("unitedhealthgroup","unitedhealthgroup",        5, "UnitedHealth Group"),
        ("cvs",              "cvs",                     5,  "CVS Health"),
        ("aetna",            "aetna",                   5,  "Aetna"),
        ("cigna",            "cigna",                   5,  "Cigna"),
        ("humana",           "humana",                  5,  "Humana"),
        ("elevancehealth",   "elevancehealth",          5,  "Elevance Health"),
        ("jnj",              "jnjjobs",                 5,  "Johnson & Johnson"),
        ("pfizer",           "pfizercareers",           5,  "Pfizer"),
        ("abbvie",           "abbvie",                  5,  "AbbVie"),
        ("amgen",            "amgen",                   5,  "Amgen"),
        ("genentech",        "genentech",               5,  "Genentech"),
        ("lilly",            "lilly",                   5,  "Eli Lilly"),
        ("medtronic",        "medtronic",               5,  "Medtronic"),
        ("stryker",          "stryker",                 5,  "Stryker"),
        # Consulting & Professional Services
        ("deloitte",         "deloitte",                5,  "Deloitte"),
        ("pwc",              "pwc",                     5,  "PwC"),
        ("accenture",        "accenture",               5,  "Accenture"),
        ("kpmg",             "kpmg",                    5,  "KPMG"),
        ("ey",               "ey",                      5,  "Ernst & Young"),
        ("mckinsey",         "mckinsey",                5,  "McKinsey"),
        ("bcg",              "bcg",                     5,  "BCG"),
        # Telecom & Media
        ("att",              "att",                     5,  "AT&T"),
        ("verizon",          "verizon",                 5,  "Verizon"),
        ("tmobile",          "tmobile",                 5,  "T-Mobile"),
        ("comcast",          "comcast",                 5,  "Comcast"),
        ("disney",           "disney",                  5,  "Disney"),
        ("warnermedia",      "warnermedia",             5,  "Warner Bros Discovery"),
        ("nbcuniversal",     "nbcuniversal",            5,  "NBCUniversal"),
        # Defense & Gov Contractors
        ("boeing",           "boeing",                  5,  "Boeing"),
        ("lockheedmartin",   "lockheedmartin",          5,  "Lockheed Martin"),
        ("raytheon",         "raytheon",                5,  "Raytheon"),
        ("northropgrumman",  "northropgrumman",         5,  "Northrop Grumman"),
        ("generalatomics",   "generalatomics",          5,  "General Atomics"),
        ("leidos",           "leidos",                  5,  "Leidos"),
        ("saic",             "saic",                    5,  "SAIC"),
        ("bah",              "bah",                     5,  "Booz Allen Hamilton"),
        ("l3harris",         "l3harris",                5,  "L3Harris"),
        # Manufacturing & Industrial
        ("ge",               "ge",                      5,  "GE"),
        ("honeywell",        "honeywell",               5,  "Honeywell"),
        ("3m",               "3m",                      5,  "3M"),
        ("caterpillar",      "caterpillar",             5,  "Caterpillar"),
        ("emerson",          "emerson",                 5,  "Emerson Electric"),
    ]
    jobs = []
    with ThreadPoolExecutor(max_workers=25) as pool:
        futures = {pool.submit(_workday, t, b, v, n): n for t, b, v, n in companies}
        for f in as_completed(futures):
            try:
                jobs.extend(f.result())
            except Exception:
                pass
    return jobs

def fetch_greenhouse_bulk():
    """80+ companies on Greenhouse."""
    slugs = [
        ("airbnb","Airbnb"),("databricks","Databricks"),
        ("snowflakecomputing","Snowflake"),("confluent","Confluent"),
        ("dbtlabs","dbt Labs"),("fivetran","Fivetran"),
        ("hightouch","Hightouch"),("amplitude","Amplitude"),
        ("segment","Segment"),("coinbase","Coinbase"),
        ("robinhood","Robinhood"),("plaid","Plaid"),
        ("brex","Brex"),("chime","Chime"),("gusto","Gusto"),
        ("rippling","Rippling"),("lattice","Lattice"),
        ("asana","Asana"),("notion","Notion"),("vercel","Vercel"),
        ("hashicorp","HashiCorp"),("datadog","Datadog"),
        ("elastic","Elastic"),("mongodb","MongoDB"),
        ("pinecone","Pinecone"),("cloudflare","Cloudflare"),
        ("lyft","Lyft"),("doordash","DoorDash"),("instacart","Instacart"),
        ("shopify","Shopify"),("klaviyo","Klaviyo"),("twilio","Twilio"),
        ("stripe","Stripe"),("faire","Faire"),("palantir","Palantir"),
        ("github","GitHub"),("anthropic","Anthropic"),
        ("anyscale","Anyscale"),("prefect","Prefect"),
        ("dagster","Dagster"),("dremio","Dremio"),
        ("starburst","Starburst"),("clickhouse","ClickHouse"),
        ("motherduck","MotherDuck"),("tecton","Tecton"),
        ("airbyte","Airbyte"),("monte_carlo","Monte Carlo"),
        ("metabase","Metabase"),("hex","Hex"),
        ("sigma","Sigma Computing"),("thoughtspot","ThoughtSpot"),
        ("atlan","Atlan"),("alation","Alation"),("collibra","Collibra"),
        ("immuta","Immuta"),("soda","Soda"),("datafold","Datafold"),
        ("acceldata","Acceldata"),("fiddler","Fiddler"),
        ("arize","Arize AI"),("superwise","Superwise"),
        ("imply","Imply"),("startree","StarTree"),
        ("tinybird","Tinybird"),("estuary","Estuary"),
        ("acryl","Acryl Data"),("aporia","Aporia"),
        ("zenml","ZenML"),("bentoml","BentoML"),
        ("weaviate","Weaviate"),("milvus","Zilliz/Milvus"),
        ("qdrant","Qdrant"),("chroma","Chroma"),
        ("deepset","deepset"),("comet","Comet ML"),
        ("weights-biases","Weights & Biases"),("determined","Determined AI"),
        ("scale","Scale AI"),("cohere","Cohere"),
        ("block-external-board","Block/Square"),
        ("uber","Uber"),("square","Square"),
        ("cloudkitchens","CloudKitchens"),("gopuff","GoPuff"),
        ("wayfair","Wayfair (GH)"),("chewy","Chewy"),
        ("grubhub","Grubhub"),("yelp","Yelp"),
        ("duolingo","Duolingo (GH)"),("pinterest","Pinterest"),
    ]
    jobs = []
    with ThreadPoolExecutor(max_workers=25) as pool:
        futures = {pool.submit(_greenhouse, slug, name): name for slug, name in slugs}
        for f in as_completed(futures):
            try:
                jobs.extend(f.result())
            except Exception:
                pass
    return jobs

def fetch_lever_bulk():
    """50+ companies on Lever."""
    slugs = [
        ("netflix","Netflix"),("reddit","Reddit"),("figma","Figma"),
        ("canva","Canva"),("airtable","Airtable"),("coda","Coda"),
        ("retool","Retool"),("discord","Discord"),("duolingo","Duolingo"),
        ("miro","Miro"),("loom","Loom"),("brex","Brex"),
        ("deel","Deel"),("pagerduty","PagerDuty"),("splunk","Splunk"),
        ("atlassian","Atlassian"),("circleci","CircleCI"),
        ("buildkite","Buildkite"),("firehydrant","FireHydrant"),
        ("rootly","Rootly"),("opentable","OpenTable"),
        ("opendoor","Opendoor"),("redfin","Redfin"),("compass","Compass"),
        ("sofi","SoFi"),("nerdwallet","NerdWallet"),
        ("creditkarma","Credit Karma"),("workiva","Workiva"),
        ("zuora","Zuora"),("chargebee","Chargebee"),
        ("clari","Clari"),("gong","Gong"),("salesloft","Salesloft"),
        ("outreach","Outreach"),("apollo","Apollo.io"),
        ("drift","Drift"),("intercom","Intercom"),
        ("loom","Loom"),("linear","Linear"),
        ("shortcut","Shortcut"),("launchdarkly","LaunchDarkly"),
        ("honeycomb","Honeycomb"),("lightstep","LightStep"),
        ("chronosphere","Chronosphere"),("grafana","Grafana Labs"),
        ("newrelic","New Relic"),("sumologic","Sumo Logic"),
        ("papertrail","Papertrail"),("loggly","Loggly"),
        ("veritone","Veritone"),("primer","Primer AI"),
    ]
    jobs = []
    with ThreadPoolExecutor(max_workers=20) as pool:
        futures = {pool.submit(_lever, slug, name): name for slug, name in slugs}
        for f in as_completed(futures):
            try:
                jobs.extend(f.result())
            except Exception:
                pass
    return jobs

def fetch_ashby_bulk():
    """40+ companies on Ashby HQ."""
    slugs = [
        ("ramp","Ramp"),("perplexity","Perplexity"),("cursor","Cursor"),
        ("replit","Replit"),("warp","Warp"),("raycast","Raycast"),
        ("posthog","PostHog"),("codeium","Codeium"),
        ("together","Together AI"),("mistral","Mistral AI"),
        ("arc","Arc Browser"),("clay","Clay"),
        ("evidence","Evidence"),("rill","Rill Data"),
        ("fennel","Fennel"),("chalk","Chalk"),
        ("continual","Continual"),("vectara","Vectara"),
        ("qdrant","Qdrant (Ashby)"),("marqo","Marqo"),
        ("lakera","Lakera"),("superconductive","Great Expectations"),
        ("brainbase","Brainbase"),("truera","TruEra"),
        ("weights-biases","W&B (Ashby)"),("neptune","Neptune.ai"),
        ("comet","Comet (Ashby)"),("gantry","Gantry"),
        ("whylabs","WhyLabs (Ashby)"),("aporia","Aporia (Ashby)"),
        ("scale","Scale (Ashby)"),("labelbox","Labelbox"),
        ("segments","Segments.ai"),("roboflow","Roboflow"),
        ("superb-ai","Superb AI"),("encord","Encord"),
        ("aquarium","Aquarium"),("galileo","Galileo"),
        ("arthur","Arthur AI"),("fiddler","Fiddler (Ashby)"),
    ]
    jobs = []
    with ThreadPoolExecutor(max_workers=15) as pool:
        futures = {pool.submit(_ashby, slug, name): name for slug, name in slugs}
        for f in as_completed(futures):
            try:
                jobs.extend(f.result())
            except Exception:
                pass
    return jobs

def fetch_smartrecruiters_bulk():
    """Companies on SmartRecruiters."""
    slugs = [
        ("intel","Intel"),("lenovo","Lenovo"),("samsung","Samsung America"),
        ("siemens","Siemens USA"),("abb","ABB"),("bosch","Bosch USA"),
        ("philips","Philips USA"),("nokia","Nokia"),("ericsson","Ericsson"),
        ("capgemini","Capgemini"),("cognizant","Cognizant"),
        ("infosys","Infosys"),("wipro","Wipro"),
        ("hcltech","HCL Technologies"),("dxc","DXC Technology"),
        ("epam","EPAM Systems"),("globant","Globant"),("endava","Endava"),
        ("tibco","TIBCO"),("opentext","OpenText"),("hpe","HPE"),
        ("microfocus","Micro Focus"),("sap","SAP (SR)"),
    ]
    jobs = []
    with ThreadPoolExecutor(max_workers=10) as pool:
        futures = {pool.submit(_smartrecruiters, slug, name): name for slug, name in slugs}
        for f in as_completed(futures):
            try:
                jobs.extend(f.result())
            except Exception:
                pass
    return jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MASTER SOURCE REGISTRY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ALL_SOURCES = [
    # RSS Feeds
    ("Indeed RSS",             fetch_indeed),
    ("ZipRecruiter RSS",       fetch_ziprecruiter),
    ("Dice RSS",               fetch_dice),
    ("SimplyHired RSS",        fetch_simplyhired),
    ("WeWorkRemotely RSS",     fetch_weworkremotely),
    ("CareerBuilder RSS",      fetch_careerbuilder),
    # Job Board APIs
    ("Remotive",               fetch_remotive),
    ("Arbeitnow",              fetch_arbeitnow),
    ("The Muse",               fetch_themuse),
    ("RemoteOK",               fetch_remoteok),
    ("Himalayas",              fetch_himalayas),
    ("USAJobs",                fetch_usajobs),
    ("Adzuna",                 fetch_adzuna),
    ("Wellfound/AngelList",    fetch_wellfound),
    ("YC Work at a Startup",   fetch_yc_jobs),
    ("LinkedIn",               fetch_linkedin_jobs),
    # Major Companies Direct
    ("Amazon Jobs",            fetch_amazon),
    ("Google Careers",         fetch_google),
    ("Microsoft Careers",      fetch_microsoft),
    ("Meta Careers",           fetch_meta),
    ("IBM Careers",            fetch_ibm),
    ("Oracle Careers",         fetch_oracle),
    # Bulk ATS Platform Scanners
    ("Workday (120+ companies)",      fetch_workday_bulk),
    ("Greenhouse (80+ companies)",    fetch_greenhouse_bulk),
    ("Lever (50+ companies)",         fetch_lever_bulk),
    ("Ashby (40+ companies)",         fetch_ashby_bulk),
    ("SmartRecruiters (25+ companies)", fetch_smartrecruiters_bulk),
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NOTIFICATIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_desktop(j: dict):
    if not DESKTOP_AVAILABLE:
        return
    try:
        desktop_notify.notify(
            title=f"ğŸš¨ {j['title']}",
            message=f"{j['company']} | {j['location'] or 'See posting'}\n{j['source']}",
            timeout=10)
    except Exception:
        pass

def send_email(jobs: list):
    if not SMTP_USER or not SMTP_PASS:
        if jobs:
            log.warning("Email not configured â€” set SMTP_USER and SMTP_PASS in .env")
        return
    n = len(jobs)
    subject = f"ğŸš¨ {n} New Data Engineer Job{'s' if n>1 else ''} â€” Apply NOW (first 10!)"
    html = (f"<html><body style='font-family:Arial,sans-serif;max-width:720px;margin:auto'>"
            f"<h2 style='color:#1a73e8'>ğŸš¨ {n} New Data Engineer Job{'s' if n>1 else ''}</h2>"
            f"<p style='color:#666'>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            f" â€” Apply fast to be in the first 10 applicants!</p><hr>")
    for j in jobs:
        html += (
            f"<div style='border-left:4px solid #1a73e8;padding:10px 16px;"
            f"margin:14px 0;background:#f8f9fa'>"
            f"<h3 style='margin:0 0 4px'>"
            f"<a href='{j['url']}' style='color:#1a73e8;text-decoration:none'>"
            f"{j['title']}</a></h3>"
            f"<p style='margin:2px 0'>ğŸ¢ <b>{j['company']}</b></p>"
            f"<p style='margin:2px 0;color:#555'>ğŸ“ {j['location'] or 'See posting'}</p>"
            f"<p style='margin:2px 0;color:#888;font-size:.85em'>"
            f"Source: {j['source']}</p>"
            f"<a href='{j['url']}' style='display:inline-block;margin-top:8px;"
            f"padding:6px 16px;background:#1a73e8;color:#fff;border-radius:4px;"
            f"text-decoration:none;font-size:.9em'>Apply Now â†’</a></div>")
    html += "</body></html>"
    msg = MIMEMultipart("alternative")
    msg["From"], msg["To"], msg["Subject"] = SMTP_USER, ALERT_EMAIL, subject
    msg.attach(MIMEText(html, "html"))
    try:
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as srv:
            srv.starttls()
            srv.login(SMTP_USER, SMTP_PASS)
            srv.sendmail(SMTP_USER, ALERT_EMAIL, msg.as_string())
        log.info(f"ğŸ“§ Email sent â†’ {ALERT_EMAIL} ({n} new job{'s' if n>1 else ''})")
    except Exception as ex:
        log.error(f"Email failed: {ex}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CORE LOOP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_and_notify():
    log.info(f"ğŸ” Scanning {len(ALL_SOURCES)} sources in parallel...")
    seen = load_seen()
    new_jobs = []

    with ThreadPoolExecutor(max_workers=30) as pool:
        futures = {pool.submit(fn): name for name, fn in ALL_SOURCES}
        for future in as_completed(futures):
            name = futures[future]
            try:
                for j in future.result():
                    if not j.get("url"):
                        continue
                    jid = make_id(j["source"], j["url"])
                    if jid not in seen:
                        seen.add(jid)
                        new_jobs.append(j)
                        log.info(f"  ğŸ†• [{j['source']}] {j['title']} @ {j['company']}")
                        send_desktop(j)   # Instant popup per job
            except Exception as ex:
                log.debug(f"  âœ— {name}: {ex}")

    save_seen(seen)

    if new_jobs:
        send_email(new_jobs)
        log.info(f"âœ… {len(new_jobs)} new job(s) found â€” notifications sent.")
    else:
        log.info("âœ… Scan complete â€” no new jobs.")

    return new_jobs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Real-Time Data Engineer Job Monitor")
    parser.add_argument("--once", action="store_true", help="Scan once and exit")
    parser.add_argument("--test", action="store_true",
                        help="Send test email + desktop notification")
    args = parser.parse_args()

    if args.test:
        dummy = {"title": "Senior Data Engineer [TEST]", "company": "ACME Corp",
                 "location": "Remote USA", "url": "https://example.com/job",
                 "source": "Test", "posted_at": datetime.now().isoformat()}
        log.info("Sending test notifications...")
        send_desktop(dummy)
        send_email([dummy])
        log.info("Done â€” check your email and desktop.")

    elif args.once:
        check_and_notify()

    else:
        log.info(f"ğŸš€ Job Monitor v3.0 started")
        log.info(f"   Sources  : {len(ALL_SOURCES)} ({sum(1 for n,_ in ALL_SOURCES)} groups)")
        log.info(f"   Coverage : 500+ companies, all major ATS platforms")
        log.info(f"   Interval : every {CHECK_INTERVAL} minutes")
        log.info(f"   Alert to : {ALERT_EMAIL or 'âš   Set ALERT_EMAIL in .env'}")
        log.info("   Press Ctrl+C to stop\n")
        check_and_notify()
        schedule.every(CHECK_INTERVAL).minutes.do(check_and_notify)
        while True:
            schedule.run_pending()
            time.sleep(30)
